# Nerfstudio Pipeline Guide
# =========================

## Prerequisites
# --------------
sudo apt update && apt install ffmpeg colmap -y
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118


## Docker Setup (REQUIRED for Gaussian Splatting)
# ------------------------------------------------
docker pull ghcr.io/nerfstudio-project/nerfstudio:latest

# Start Docker container
docker run --gpus all \
  -v /mnt/c/python_work/git/data/nerfstudio:/workspace \
  -v ~/.cache:/home/user/.cache \
  -p 7007:7007 --rm -it --shm-size=12gb \
  ghcr.io/nerfstudio-project/nerfstudio:latest bash


## Complete Pipeline: Video → Training Dataset with Masks
# ---------------------------------------------------------

# Step 1: Process video (extract frames + COLMAP poses)
ns-process-data video --data video/MY_VIDEO.mp4 --output-dir data/my_object

# Step 2: Train Gaussian Splatting (inside Docker)
cd /workspace
ns-train splatfacto --data data/my_object --output-dir data/my_object --viewer.start-on-train False

# Step 3: Export to .ply
ns-export gaussian-splat --load-config data/my_object/splatfacto/<timestamp>/config.yml --output-dir exports/combined_gaussian/

# Step 4: (Optional) Crop in SuperSplat
# Download from: https://github.com/playcanvas/super-splat/releases
# Import .ply → Select unwanted parts → Press 'I' → Delete → Export

# Step 5: Generate dataset with RGB + Mask + Camera calibration
cd /workspace/exports/combined_gaussian
pip3 install ultralytics
python render_yolo.py --ply splat.ply --views 200 --out training_dataset

# Output: training_dataset/ with render_XXXX.png, render_XXXX_mask.png, render_XXXX_camera.json


## render_yolo.py Usage
# ---------------------

# Single view (testing)
python render_yolo.py

# Multiple views
python render_yolo.py --views 100

# Full options
python render_yolo.py --ply FILE.ply --out DIR --views 200 --size 1920 1080 --model yolov8m-seg.pt --conf 0.1


## Output Format
# --------------

# Each view generates 3 files:
# - render_XXXX.png        → RGB image
# - render_XXXX_mask.png   → Binary mask (white=object, black=background)
# - render_XXXX_camera.json → {"pos": [x,y,z], "look_at": [x,y,z], "K": [[...]], "focal": 2058, "size": [w,h]}


## How It Works
# -------------

# 1. Load .ply (Gaussian parameters: position, color, scale, rotation, opacity)
# 2. Calculate camera positions orbiting the object (360° horizontal, ±30° vertical)
# 3. Render using gsplat.rasterization (GPU-accelerated)
# 4. Segment objects using YOLOv8 instance segmentation
# 5. Save RGB, mask, and camera calibration


## Troubleshooting
# ----------------

# Wrong mask (captures platform instead of object):
#   → Use render_yolo.py with --conf 0.1
#   → OR crop unwanted parts in SuperSplat first

# YOLO detects only partial object:
#   → Lower confidence: --conf 0.05
#   → Larger model: --model yolov8m-seg.pt

# pip not found in Docker:
#   → Use: python3 -m pip install ultralytics

# PyTorch checkpoint errors:
#   → pip install torch==2.4.0 torchvision==0.19.0


## Other Useful Commands
# ----------------------

# Process images instead of video
ns-process-data images --data IMAGE_FOLDER --output-dir data/my_object

# View trained model
ns-viewer --load-config PATH/config.yml  # Open http://localhost:7007

# Resume training
ns-train splatfacto --data DATA --output-dir OUT --load-dir OUT/nerfstudio_models

# Find checkpoints
ls -lt data/my_object/splatfacto/

# Monitor GPU
nvidia-smi
watch -n 1 nvidia-smi
